{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e57b936-a773-4338-8141-4ae978fce636",
   "metadata": {},
   "source": [
    "# Import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "869e25ab-2a74-4477-8ba8-049fe16bfcb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Algebra and numerical computations\n",
    "import scipy  # Acientific and technical computing\n",
    "from sklearn import preprocessing  # Utilities for machine learning\n",
    "from sklearn_som.som import SOM  # Self-Organizing Map implementation\n",
    "from scipy import stats  # Statistical functions\n",
    "from scipy.io import arff  # For reading ARFF (Attribute-Relation File Format) files\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis for dimensionality reduction\n",
    "from sklearn.naive_bayes import GaussianNB  # Gaussian Naive Bayes classifier\n",
    "from sklearn.metrics import accuracy_score  # Function to calculate accuracy score\n",
    "from sklearn import metrics  # Various machine learning metrics\n",
    "import os  # Miscellaneous operating system interfaces\n",
    "from matplotlib import pyplot as plt  # Plotting library\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'  # Set Keras backend to TensorFlow\n",
    "import tensorflow as tf  # Deep learning framework\n",
    "np.float = float  # Small correction to ensure compatibility with skmultiflow\n",
    "from keras.datasets import cifar10  # CIFAR-10 dataset\n",
    "from keras.datasets import mnist  # MNIST dataset\n",
    "from keras.utils import to_categorical  # For converting labels to categorical format\n",
    "from random import randrange  # Generate random numbers\n",
    "from sklearn.utils import shuffle  # Utility to shuffle datasets\n",
    "from tqdm import tqdm  # Progress bar for loops\n",
    "import math  # Mathematical functions\n",
    "from keras.optimizers import SGD, Adam  # Stochastic Gradient Descent and Adam optimizers\n",
    "from keras.models import Sequential  # Sequential model in Keras\n",
    "from keras import layers  # Layers for building neural networks\n",
    "import pandas as pd  # Data manipulation and analysis library\n",
    "from datetime import datetime as dt  # Date and time manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712dad22-5c6d-4d32-af7d-868a1bf9d749",
   "metadata": {},
   "source": [
    "# SOM projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03487da5-e971-44bc-8e8f-9429250f61d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is based in the code of Riley Smith (skearn-som), with some modifications for managing the distances.\n",
    "\"\"\"\n",
    "\n",
    "class LargeSpaceSOM():\n",
    "    \"\"\"\n",
    "    Default is a 2-D, rectangular grid self-organizing map class using Numpy.\n",
    "    \"\"\"\n",
    "    def __init__(self, m=5, n=5, dim=3, lr=1, sigma=1, max_iter=300, dist_type=\"Cosine\",\n",
    "                    random_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        m : int, default=3\n",
    "            The shape along dimension 0 (vertical) of the SOM.\n",
    "        n : int, default=3\n",
    "            The shape along dimesnion 1 (horizontal) of the SOM.\n",
    "        dim : int, default=3\n",
    "            The dimensionality (number of features) of the input space.\n",
    "        lr : float, default=1\n",
    "            The initial step size for updating the SOM weights.\n",
    "        sigma : float, optional\n",
    "            Optional parameter for magnitude of change to each weight. Does not\n",
    "            update over training (as does learning rate). Higher values mean\n",
    "            more aggressive updates to weights.\n",
    "        max_iter : int, optional\n",
    "            Optional parameter to stop training if you reach this many\n",
    "            interation.\n",
    "        random_state : int, optional\n",
    "            Optional integer seed to the random number generator for weight\n",
    "            initialization. This will be used to create a new instance of Numpy's\n",
    "            default random number generator (it will not call np.random.seed()).\n",
    "            Specify an integer for deterministic results.\n",
    "        \"\"\"\n",
    "        # Initialize descriptive features of SOM\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.shape = (m, n)\n",
    "        self.initial_lr = lr\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.max_iter = max_iter\n",
    "        self.dist_type=dist_type\n",
    "\n",
    "        # Initialize weights\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        rng = np.random.default_rng(random_state)\n",
    "        self.weights = rng.normal(size=(m * n, dim))\n",
    "        self._locations = self._get_locations(m, n)\n",
    "\n",
    "        # Set after fitting\n",
    "        self._inertia = None\n",
    "        self._n_iter_ = None\n",
    "        self._trained = False\n",
    "        \n",
    "        print(\"Creation of the SOM grid\")\n",
    "        print(\"Max of iterations\", self.max_iter)\n",
    "        print(\"Distance\", self.dist_type)\n",
    "        \n",
    "\n",
    "        # DONE\n",
    "    def _get_locations(self, m, n):\n",
    "        \"\"\"\n",
    "        Return the indices of an m by n array.\n",
    "        \"\"\"\n",
    "        return np.argwhere(np.ones(shape=(m, n))).astype(np.int64)\n",
    "\n",
    "        #\n",
    "        # Critical funtion\n",
    "    def _find_bmu(self, x):\n",
    "        \"\"\"\n",
    "        Find the index of the best matching unit for the input vector x.\n",
    "        \"\"\"\n",
    "        # Stack x to have one row per weight\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "        # Calculate distance between x and each weight\n",
    "        if self.dist_type==\"Cosine\":\n",
    "            # Cosine measure: invariant to rotation, independent to vector length.\n",
    "            #distance=np.sum(np.diag(np.outer(x_stack,self.weights)))/(scipy.linalg.norm(x_stack,2)*scipy.linalg.norm(self.weights,2))           \n",
    "            #print(\"Cosine distance computation\")\n",
    "            #print(\"x_stack shape, self.weights shape\",x_stack.shape, self.weights.shape)\n",
    "            distance=np.ndarray(shape=(1,x_stack.shape[0]))\n",
    "            #print(\"Distance\", distance)\n",
    "            for i in range(x_stack.shape[0]):\n",
    "                # Scipy didn't work requires normalized vectors.\n",
    "                #distance[0,i]=metrics.pairwise.cosine_similarity([x_stack[i,:]], [self.weights[i,:]])\n",
    "                distance[0,i]=np.dot(x_stack[i,:],self.weights[i,:])/((np.linalg.norm(x_stack[i,:]))*(np.linalg.norm(self.weights[i,:])))\n",
    "                #print(\"i\", i, \"Obtained distance:\", distance[0,i])\n",
    "                # Compute distance between raw input and weights of each neurons.\n",
    "            \n",
    "            #Cosine measure can give negative values, so you can compare the relative strengh \n",
    "            #of 2 cosine similarities by looking at the absolute values,\n",
    "            #just like how you would compare the absolute values of 2 Pearson correlations.\n",
    "            output_value=np.argmin(np.abs(distance[0,:]))\n",
    "                        \n",
    "        else:\n",
    "            #  Frobenius norm of (a-b)\n",
    "            # Compute distance between raw input and weights of each neurons.\n",
    "            distance = np.linalg.norm(x_stack - self.weights, axis=1)\n",
    "            #print(\"distance shape\",distance.shape)\n",
    "            output_value=np.argmin(distance)\n",
    "                \n",
    "        # Find index of best matching unit\n",
    "        #print(\"Distance:\", distance)\n",
    "        #print(\"Output value:\", output_value)\n",
    "        \n",
    "        return output_value\n",
    "\n",
    "    def step(self, x):\n",
    "        \"\"\"\n",
    "        Do one step of training on the given input vector.\n",
    "        \"\"\"\n",
    "        # Stack x to have one row per neuron weight, it create a table with multiple row but all with same content.\n",
    "        x_stack = np.stack([x]*(self.m*self.n), axis=0)\n",
    "\n",
    "        # Get index of best matching unit\n",
    "        bmu_index = self._find_bmu(x)\n",
    "        \n",
    "        # Find location of best matching unit\n",
    "        bmu_location = self._locations[bmu_index,:]\n",
    "\n",
    "        # Find square distance from each weight to the BMU\n",
    "        stacked_bmu = np.stack([bmu_location]*(self.m*self.n), axis=0)\n",
    "        # distance among locations.\n",
    "        bmu_distance = np.sum(np.power(self._locations.astype(np.float64) - stacked_bmu.astype(np.float64), 2), axis=1)\n",
    "\n",
    "        # Compute update neighborhood - Mexican hat\n",
    "        neighborhood = np.exp((bmu_distance / (self.sigma ** 2)) * -1)\n",
    "        local_step = self.lr * neighborhood\n",
    "\n",
    "        # Stack local step to be proper shape for update\n",
    "        local_multiplier = np.stack([local_step]*(self.dim), axis=1)\n",
    "\n",
    "        # Multiply by difference between input and weights\n",
    "        delta = local_multiplier * (x_stack - self.weights)\n",
    "\n",
    "        # Update weights\n",
    "        self.weights += delta\n",
    "\n",
    "    def _compute_point_intertia(self, x):\n",
    "        \"\"\"\n",
    "        Compute the inertia of a single point. Inertia defined as distance\n",
    "        from point to closest cluster center (BMU)\n",
    "        \"\"\"\n",
    "        # Find BMU\n",
    "        bmu_index = self._find_bmu(x)\n",
    "        bmu = self.weights[bmu_index]\n",
    "        # Compute distance (just euclidean distance) from x to bmu\n",
    "        \n",
    "        if self.dist_type==\"Cosine\":\n",
    "            distance=np.abs(np.dot(x,bmu)/((np.linalg.norm(x))*(np.linalg.norm(bmu))))\n",
    "        else:\n",
    "            distance = np.linalg.norm(x - bmu)\n",
    "           \n",
    "        return distance\n",
    "\n",
    "    def fit(self, X, epochs=1, shuffle=False):\n",
    "        \"\"\"\n",
    "        Take data (a tensor of type float64) as input and fit the SOM to that\n",
    "        data for the specified number of epochs.\n",
    "        \"\"\"\n",
    "        # Count total number of iterations\n",
    "        global_iter_counter = 0\n",
    "        n_samples = X.shape[0]\n",
    "        total_iterations = np.minimum(epochs * n_samples, self.max_iter)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Break if past max number of iterations\n",
    "            if global_iter_counter > self.max_iter:\n",
    "                break\n",
    "            # for streaming data has non sense to shuffle.\n",
    "            if shuffle:\n",
    "                rng = np.random.default_rng(self.random_state)\n",
    "                indices = rng.permutation(n_samples)\n",
    "            else:\n",
    "                indices = np.arange(n_samples)\n",
    "\n",
    "            # Train\n",
    "            for idx in indices:\n",
    "                # Break if past max number of iterations\n",
    "                if global_iter_counter > self.max_iter:\n",
    "                    break\n",
    "                # Do one step of training\n",
    "                self.step(X[idx])\n",
    "                # Update learning rate\n",
    "                global_iter_counter += 1\n",
    "                self.lr = (1 - (global_iter_counter / total_iterations)) * self.initial_lr\n",
    "\n",
    "                \n",
    "        print(\"After epochs\")\n",
    "        # Compute inertia\n",
    "        inertia = np.sum(np.array([float(self._compute_point_intertia(x)) for x in X]))\n",
    "        self._inertia_ = inertia\n",
    "        print(\"Inertia:\",inertia)\n",
    "\n",
    "        # Set n_iter_ attribute\n",
    "        self._n_iter_ = global_iter_counter\n",
    "\n",
    "        # Set trained flag\n",
    "        self._trained = True\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster for each element in X.\n",
    "        \"\"\"\n",
    "        # Check to make sure SOM has been fit\n",
    "        if not self._trained:\n",
    "            raise NotImplementedError('SOM object has no predict() method until after calling fit().')\n",
    "\n",
    "        # Make sure X has proper shape\n",
    "        assert len(X.shape) == 2, f'X should have two dimensions, not {len(X.shape)}'\n",
    "        assert X.shape[1] == self.dim, f'This SOM has dimesnion {self.dim}. Received input with dimension {X.shape[1]}'\n",
    "\n",
    "        labels = np.array([self._find_bmu(x) for x in X])\n",
    "        return labels\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data X into cluster distance space.\n",
    "        \"\"\"\n",
    "        # Stack data and cluster centers\n",
    "        X_stack = np.stack([X]*(self.m*self.n), axis=1)\n",
    "        cluster_stack = np.stack([self.weights]*X.shape[0], axis=0)\n",
    "\n",
    "        # Compute difference\n",
    "        diff = X_stack - cluster_stack\n",
    "\n",
    "        # Take and return norm\n",
    "        return np.linalg.norm(diff, axis=2)\n",
    "\n",
    "    def fit_predict(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Convenience method for calling fit(X) followed by predict(X).\n",
    "        \"\"\"\n",
    "        # Fit to data\n",
    "        self.fit(X, **kwargs)\n",
    "\n",
    "        # Return predictions\n",
    "        return self.predict(X)\n",
    "\n",
    "    def fit_transform(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Convenience method for calling fit(X) followed by transform(X). Unlike\n",
    "        in sklearn, this is not implemented more efficiently (the efficiency is\n",
    "        the same as calling fit(X) directly followed by transform(X)).\n",
    "        \"\"\"\n",
    "        # Fit to data\n",
    "        self.fit(X, **kwargs)\n",
    "\n",
    "        # Return points in cluster distance space\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def distance_Clusters(self, x):\n",
    "        \"\"\"\n",
    "        Distance to clusters using cosine\n",
    "        \"\"\"\n",
    "\n",
    "        distance=np.ndarray(shape=(1,self.weights.shape[0]))\n",
    "        #print(\"Distance\", distance)\n",
    "        for i in range(self.weights.shape[0]):\n",
    "            distance[0,i]=np.dot(x,self.weights[i,:])/((np.linalg.norm(x))*(np.linalg.norm(self.weights[i,:])))\n",
    "                \n",
    "        # Return points in cluster distance space\n",
    "        return distance\n",
    "\n",
    "    def distance_Euclidean_Clusters(self, x):\n",
    "        \"\"\"\n",
    "        Distance to clusters using cosine\n",
    "        \"\"\"\n",
    "        distance=np.ndarray(shape=(1,self.weights.shape[0]))\n",
    "        for i in range(self.weights.shape[0]):\n",
    "            # Return points in cluster distance space\n",
    "            difference = x - self.weights[i,:]\n",
    "            # Compute the Euclidean distance\n",
    "            distance[0,i]=np.linalg.norm(difference)\n",
    "            \n",
    "        return distance\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def cluster_centers_(self):\n",
    "        return self.weights.reshape(self.m, self.n, self.dim)\n",
    "\n",
    "    @property\n",
    "    def inertia_(self):\n",
    "        if self._inertia_ is None:\n",
    "            raise AttributeError('SOM does not have inertia until after calling fit()')\n",
    "        return self._inertia_\n",
    "\n",
    "    @property\n",
    "    def n_iter_(self):\n",
    "        if self._n_iter_ is None:\n",
    "            raise AttributeError('SOM does not have n_iter_ attribute until after calling fit()')\n",
    "        return self._n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b57d6-6223-4cc9-97b2-59029447054a",
   "metadata": {},
   "source": [
    "# Creation of study cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44b7d569-dc98-413e-9814-14164a7ef018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "#\n",
    "def extract(A, B, L, lenght):\n",
    "    # Find the elements in A that are in L\n",
    "    mask = np.isin(B, L)\n",
    "    # Get the indices of these elements\n",
    "    indices = np.where(mask)[0]\n",
    "    # Randomly select 1000 indices\n",
    "    selected_indices = np.random.choice(indices, size=lenght, replace=False)\n",
    "    # Get the selected elements\n",
    "    selected_elements = A[selected_indices]\n",
    "    label_elements=B[selected_indices]\n",
    "    return selected_elements, label_elements\n",
    "\n",
    "#\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "type1=[1,3,5,7]\n",
    "type2=[0,6,9]\n",
    "type3=[2,4]\n",
    "type4=[8]\n",
    "#\n",
    "# Case study A: mixture between type 1 and type 2.\n",
    "data_Mnist_A_X = np.zeros(shape=(20000,28,28)) \n",
    "data_Mnist_A_y = np.zeros(shape=(20000))\n",
    "#\n",
    "# Case study: mixture between type 1 + 2 and type 2 + 4.\n",
    "data_Mnist_B_X = np.zeros(shape=(20000,28,28))\n",
    "data_Mnist_B_y = np.zeros(shape=(20000))\n",
    "#\n",
    "# Mixture between type 1 and type 2 and type type+type4.\n",
    "data_Mnist_C_X = np.zeros(shape=(20000,28,28))\n",
    "data_Mnist_C_y = np.zeros(shape=(20000))\n",
    "#\n",
    "####\n",
    "# Case A\n",
    "####\n",
    "length=1000\n",
    "i=0\n",
    "while i<19:\n",
    "    #\n",
    "    selX,sely=extract(X, y, type1, length)\n",
    "    data_Mnist_A_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_A_y[i*length:(i+1)*length] = sely[:]\n",
    "    #plt.imshow(data_Mnist_A_X[i*length+1,:,:])\n",
    "    #plt.savefig(data_path+\"DataSets/Img_Type_1_\"+str(i*length+1)+\".png\",bbox_inches='tight')\n",
    "    #plt.close()\n",
    "    #\n",
    "    i=i+1\n",
    "    selX,sely=extract(X, y, type2, length)\n",
    "    data_Mnist_A_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_A_y[i*length:(i+1)*length]=sely[:]\n",
    "    #print(\"type 2,  i \", i, data_Mnist_A_y[i*length])\n",
    "    #plt.imshow(data_Mnist_A_X[i*length+1,:,:])\n",
    "    #plt.savefig(data_path+\"DataSets/Img_Type_2_\"+str(i*length+1)+\".png\",bbox_inches='tight')\n",
    "    #plt.close()\n",
    "\n",
    "np.save(data_path+\"data_Mnist_A_X.npy\",data_Mnist_A_X)\n",
    "np.save(data_path+\"data_Mnist_A_y.npy\",data_Mnist_A_y)\n",
    "#plt.imshow(data_Mnist_A_X[16999,:,:])\n",
    "\n",
    "####\n",
    "# Case B\n",
    "####\n",
    "# Mixture between type 1 + 2 and type 2 + 4.\n",
    "type1a=[1,2,3,5,7]\n",
    "type2a=[0,4,6,9]\n",
    "data_Mnist_B_X = np.zeros(shape=(20000,28,28))\n",
    "data_Mnist_B_y = np.zeros(shape=(20000))\n",
    "#\n",
    "length=1000\n",
    "i=0\n",
    "while i<19:\n",
    "    #\n",
    "    selX,sely=extract(X, y, type1a, length)\n",
    "    data_Mnist_B_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_B_y[i*length:(i+1)*length]=sely[:]\n",
    "    #print(\"type 1, i \", i, data_Mnist_B_y[i*length])\n",
    "    #\n",
    "    i=i+1\n",
    "    selX,sely=extract(X, y, type2a, length)\n",
    "    data_Mnist_B_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_B_y[i*length:(i+1)*length]=sely[:]\n",
    "    #print(\"type 2,  i \", i, data_Mnist_B_y[i*length])\n",
    "    #\n",
    "\n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "np.save(data_path+\"data_Mnist_B_X.npy\",data_Mnist_B_X)\n",
    "np.save(data_path+\"data_Mnist_B_y.npy\",data_Mnist_B_y)\n",
    "\n",
    "####\n",
    "# Case C\n",
    "####\n",
    "# Mixture between type 1 + 2 and type 2 + 4.\n",
    "type4=[8]\n",
    "type1a=[1,2,3,5,7]\n",
    "type2a=[0,4,6,9]\n",
    "# Mixture between type 1 and type 2 and type type+type4.\n",
    "data_Mnist_C_X = np.zeros(shape=(20000,28,28))\n",
    "data_Mnist_C_y = np.zeros(shape=(20000))\n",
    "#\n",
    "####\n",
    "# Case C\n",
    "####\n",
    "length=500\n",
    "i=0\n",
    "while i<38:\n",
    "    #\n",
    "    selX,sely=extract(X, y, type1a, length)\n",
    "    data_Mnist_C_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_C_y[i*length:(i+1)*length]=sely[:]\n",
    "    #print(\"type SET A, i \", i, data_Mnist_C_y[i*length])\n",
    "    #\n",
    "    i=i+1\n",
    "    selX,sely=extract(X, y, type4, length)\n",
    "    data_Mnist_C_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_C_y[i*length:(i+1)*length]=sely[:]\n",
    "    #print(\"type 8,  i \", i, data_Mnist_A_y[i*length])\n",
    "    i=i+1\n",
    "    selX,sely=extract(X, y, type2a, length)\n",
    "    data_Mnist_C_X[i*length:(i+1)*length,:,:] = selX[:,:,:]\n",
    "    data_Mnist_C_y[i*length:(i+1)*length]=sely[:]\n",
    "    #print(\"type SET B, i \", i, data_Mnist_A_y[i*length])\n",
    "\n",
    "    \n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "np.save(data_path+\"data_Mnist_C_X.npy\",data_Mnist_C_X)\n",
    "np.save(data_path+\"data_Mnist_C_y.npy\",data_Mnist_C_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d11af3-8df4-4e45-8ac6-c7276fda6e67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TDA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791ee05-b959-4572-be06-4404ec606a24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Projection with SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "41c6a650-45b2-4dfe-8e58-53d885d855a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  -1.0 max: 1.0\n",
      "Creation of the SOM grid\n",
      "Max of iterations 200000\n",
      "Distance Euclidean\n",
      "After epochs\n",
      "Inertia: 43843.110217648675\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#    \n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "#\n",
    "# Case study\n",
    "file_X=\"data_Mnist_C_X.npy\"\n",
    "file_y=\"data_Mnist_C_y.npy\"\n",
    "case_A_X=np.load(data_path+file_X)\n",
    "case_A_Y=np.load(data_path+file_y)\n",
    "case_A_X_flat = case_A_X.reshape((case_A_X.shape[0], -1))\n",
    "#\n",
    "# scale to {-1,1} to better performance of SOM\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(case_A_X_flat)\n",
    "scaled_data[np.where(scaled_data[:,-1]==-1),-1]=0 \n",
    "print(\"Min: \", np.min(scaled_data), \"max:\", np.max(scaled_data))\n",
    "#\n",
    "# To define an initial windows for training the som parameters (20\\% of total).\n",
    "training_windows=round(scaled_data.shape[0]*0.2) # (1 first percent of data I use as reference windows for training the SOM)\n",
    "#\n",
    "# initialize the som latticce 10 by 10\n",
    "m=10\n",
    "n=10\n",
    "large_som_arch = LargeSpaceSOM(m=10, n=10, dim=scaled_data.shape[1]-1, dist_type=\"Euclidean\", max_iter=50*training_windows)\n",
    "# Training\n",
    "large_som_arch.fit(scaled_data[0:training_windows,0:-1])\n",
    "# Prediction of clusters:\n",
    "pred_training_data=large_som_arch.predict(scaled_data[0:training_windows,0:-1])\n",
    "dist_matrix=np.zeros(shape=(scaled_data.shape[0],n*m))\n",
    "for i in range(scaled_data.shape[0]):\n",
    "    dist_matrix[i,:]=large_som_arch.distance_Euclidean_Clusters(scaled_data[i,0:-1])\n",
    "    #dist_matrix[i,:]=large_som_arch.distance_Clusters(scaled_data[i,0:-1])\n",
    "    \n",
    "#print(\"Moment computation\")\n",
    "# Computation of the moments\n",
    "mom_matrix=np.zeros(shape=(scaled_data.shape[0],4))\n",
    "for i in range(scaled_data.shape[0]):\n",
    "    #mom_matrix[i,0]=np.mean(dist_matrix[i,:])\n",
    "    #mom_matrix[i,1]=scipy.stats.moment(dist_matrix[i,:], moment=[1])  \n",
    "    #mom_matrix[i,0]=np.std(dist_matrix[i,:])\n",
    "    #mom_matrix[i,:]=scipy.stats.moment(dist_matrix[i,:], moment=[1,2,3,4])\n",
    "    aux_moments=scipy.stats.describe(dist_matrix[i,:])\n",
    "    mom_matrix[i,0]=aux_moments.mean\n",
    "    mom_matrix[i,1]=aux_moments.variance\n",
    "    mom_matrix[i,2]=aux_moments.skewness\n",
    "    mom_matrix[i,3]=aux_moments.kurtosis\n",
    "\n",
    "    \n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "np.save(data_path+\"SOM_Case_C_dist_\"+ file_X, dist_matrix)\n",
    "np.save(data_path+\"SOM_Case_C_mom_\"+ file_X, mom_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be9dd3f0-52b0-4eaa-891b-9896c9989f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Projection with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af51e094-0075-44cd-98c7-b1121b3d7f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebbas\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\sebbas\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=16.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "#\n",
    "# Case study\n",
    "caseStudy=\"C\"\n",
    "file_X=\"data_Mnist_\"+caseStudy+\"_X.npy\"\n",
    "file_y=\"data_Mnist_\"+caseStudy+\"_y.npy\"\n",
    "case_A_X=np.load(data_path+file_X)\n",
    "case_A_Y=np.load(data_path+file_y)\n",
    "case_A_X_flat = case_A_X.reshape((case_A_X.shape[0], -1))\n",
    "#\n",
    "# Scaling in a same way than SOM\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(case_A_X_flat)\n",
    "scaled_data[np.where(scaled_data[:,-1]==-1),-1]=0 \n",
    "#\n",
    "# To define an initial windows for training the som parameters (20\\% of total).\n",
    "training_windows=round(scaled_data.shape[0]*0.2) # (1 first percent of data I use as reference windows for training the SOM)\n",
    "row_som=10\n",
    "col_som=10\n",
    "# chunk_size=50\n",
    "# steps=(scaled_data.shape[0]-training_windows)//chunk_size\n",
    "# To define an initial windows for training the som parameters (10\\% of total).\n",
    "# To define an initial windows for training the som parameters (20\\% of total).\n",
    "# Initial training\n",
    "# Compute PCA in a data stream contex\n",
    "#\n",
    "# PCA\n",
    "#\n",
    "n_components = 4\n",
    "pca_proj_data = np.zeros(shape=(scaled_data.shape[0]-training_windows, n_components))\n",
    "pca = PCA(n_components)\n",
    "pca.fit(scaled_data[0:training_windows,:])\n",
    "projPCA=pca.transform(scaled_data)\n",
    "n_clusters=row_som*col_som\n",
    "kmeans = KMeans(n_clusters, random_state=42)\n",
    "kmeans.fit(projPCA[0:training_windows,:])\n",
    "# Get the distance of each point to each cluster center\n",
    "distances = kmeans.transform(projPCA)\n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Projections/'\n",
    "np.save(data_path+\"PCA_Case_\"+caseStudy+\"_dist_data_Mnist_X.npy\",distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "827c2b92-3ed4-4e6d-a807-6d5fc9807010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Projection with KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a5a37ff-9794-4684-aec5-f1db6271e35b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebbas\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\sebbas\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=16.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/'\n",
    "# Case study\n",
    "caseStudy=\"C\"\n",
    "file_X=\"data_Mnist_\"+caseStudy+\"_X.npy\"\n",
    "file_y=\"data_Mnist_\"+caseStudy+\"_y.npy\"\n",
    "#\n",
    "case_A_X=np.load(data_path+file_X)\n",
    "case_A_Y=np.load(data_path+file_y)\n",
    "case_A_X_flat = case_A_X.reshape((case_A_X.shape[0], -1))\n",
    "#\n",
    "# scale to {-1,1} to better performance of SOM\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(case_A_X_flat)\n",
    "scaled_data[np.where(scaled_data[:,-1]==-1),-1]=0 \n",
    "#\n",
    "# To define an initial windows for training the som parameters (20\\% of total).\n",
    "training_windows=round(scaled_data.shape[0]*0.2) # (1 first percent of data I use as reference windows for training the SOM)\n",
    "row_som=10\n",
    "col_som=10\n",
    "som_epochs=5\n",
    "n_components = 4\n",
    "kpca_proj_data = np.zeros(shape=(scaled_data.shape[0]-training_windows, n_components))\n",
    "kernel_pca = KernelPCA(n_components, kernel=\"rbf\")\n",
    "kernel_pca.fit(scaled_data[0:training_windows,:])\n",
    "projKPCA=kernel_pca.transform(scaled_data)\n",
    "n_clusters=row_som*col_som\n",
    "#\n",
    "kmeans = KMeans(n_clusters, random_state=42)\n",
    "kmeans.fit(projKPCA[0:training_windows,:])\n",
    "distances = kmeans.transform(projKPCA)\n",
    "data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Projections/'\n",
    "np.save(data_path+\"KernelPCA_Case_\"+caseStudy+\"_dist_data_Mnist_X.npy\",distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22481453-47dd-4aa5-9307-97fcb0875bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading pvalues - results were made outside notebook bcs Ripser doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "924283f7-0197-48a2-af4f-3e57cb70f858",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ruptures as rpt\n",
    "plt.figure(figsize=(18,4))\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "#\n",
    "chunkSizes=[50,100,250]\n",
    "caseStudies=[\"A\",\"B\",\"C\"]\n",
    "lengths=[399,199,79]\n",
    "steps=[20,10,4]\n",
    "Methods=[\"SOM\",\"PCA\",\"KernelPCA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6e58120d-e7ca-49a3-895f-1059f0ac88e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read projections:\n",
    "#\n",
    "for trial in [0,1,2]:\n",
    "    for m in [0,1,2]:\n",
    "        for caseStudy in [\"A\",\"B\"]:\n",
    "            data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Projections/'\n",
    "            pValueSignal=np.zeros(shape=(3,lengths[trial]))\n",
    "            pValueSignal[0,:]=np.load(data_path+\"pValueSOM_\"+str(chunkSizes[trial])+\"_Case_\"+caseStudy+\".npy\")\n",
    "            pValueSignal[1,:]=np.load(data_path+\"pValuePCA_\"+str(chunkSizes[trial])+\"_Case_\"+caseStudy+\".npy\")\n",
    "            pValueSignal[2,:]=np.load(data_path+\"pValueKernelPCA_\"+str(chunkSizes[trial])+\"_Case_\"+caseStudy+\".npy\")\n",
    "            #\n",
    "            data_path = \"C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Figures/\"\n",
    "            #\n",
    "            start = 0\n",
    "            stop = lengths[trial]\n",
    "            step= steps[trial]\n",
    "            chunkSize=chunkSizes[trial]\n",
    "            # Create the numpy array\n",
    "            injectedChanges = np.arange(start, stop+step, step)-1\n",
    "            #\n",
    "            method=Methods[m]\n",
    "            signal=pValueSignal[m,:]\n",
    "            #\n",
    "            plt.figure(figsize=(18,2))\n",
    "            plt.plot(signal,\"-*\")\n",
    "            plt.xlabel(\"Chunks\")\n",
    "            plt.xlim(0,stop)\n",
    "            plt.ylabel(\"p-value\")\n",
    "            titleFig=\"Case study \" + caseStudy + \" - \"+ method + \" projections - Chunk size: \" + str(chunkSize)+ \" samples\"\n",
    "            plt.title(titleFig)\n",
    "            #ax.set_yticks([0,5])\n",
    "            plt.vlines(injectedChanges[1:], ymin=np.min(signal)-0.1, ymax=np.max(signal)+0.1, colors='green', ls='--', lw=1.3, label='vline_multiple',alpha=0.4)\n",
    "            plt.hlines(0.05,  xmin=0, xmax=stop,colors='red', ls='-', lw=1.3, alpha=0.6)\n",
    "            fileName=data_path+method+\"_Case_\"+caseStudy+str(chunkSize)+\".png\"\n",
    "            #print(fileName)\n",
    "            #plt.savefig(fileName,bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "\n",
    "# Case study C, drifts are each 500 samples.\n",
    "chunkSizes=[50,100,250]\n",
    "caseStudies=[\"C\"]\n",
    "lengths=[399,199,79]\n",
    "steps=[10,5,2]\n",
    "Methods=[\"SOM\",\"PCA\",\"KernelPCA\"]\n",
    "#\n",
    "for trial in [0,1,2]:\n",
    "    for m in [0,1,2]:\n",
    "        for caseStudy in [\"C\"]:\n",
    "            data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Projections/'\n",
    "            pValueSignal=np.zeros(shape=(3,lengths[trial]))\n",
    "            pValueSignal[0,:]=np.load(data_path+\"pValueSOM_\"+str(chunkSizes[trial])+\"_Case_\"+caseStudy+\".npy\")\n",
    "            pValueSignal[1,:]=np.load(data_path+\"pValuePCA_\"+str(chunkSizes[trial])+\"_Case_\"+caseStudy+\".npy\")\n",
    "            pValueSignal[2,:]=np.load(data_path+\"pValueKernelPCA_\"+str(chunkSizes[trial])+\"_Case_\"+caseStudy+\".npy\")\n",
    "            #\n",
    "            data_path = \"C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Figures/\"\n",
    "            #\n",
    "            start = 0\n",
    "            stop = lengths[trial]\n",
    "            step= steps[trial]\n",
    "            chunkSize=chunkSizes[trial]\n",
    "            # Create the numpy array\n",
    "            injectedChanges = np.arange(start, stop+step, step)-1\n",
    "            #\n",
    "            method=Methods[m]\n",
    "            signal=pValueSignal[m,:]\n",
    "            #\n",
    "            plt.figure(figsize=(18,2))\n",
    "            plt.plot(signal,\"-*\")\n",
    "            plt.xlabel(\"Chunks\")\n",
    "            plt.xlim(0,stop)\n",
    "            plt.ylabel(\"p-value\")\n",
    "            titleFig=\"Case study \" + caseStudy + \" - \"+ method + \" projections - Chunk size: \" + str(chunkSize)+ \" samples\"\n",
    "            plt.title(titleFig)\n",
    "            #ax.set_yticks([0,5])\n",
    "            plt.vlines(injectedChanges[1:], ymin=np.min(signal)-0.1, ymax=np.max(signal)+0.1, colors='green', ls='--', lw=1.3, label='vline_multiple',alpha=0.4)\n",
    "            plt.hlines(0.05,  xmin=0, xmax=stop,colors='red', ls='-', lw=1.3, alpha=0.6)\n",
    "            fileName=data_path+method+\"_Case_\"+caseStudy+str(chunkSize)+\".png\"\n",
    "            #print(fileName)\n",
    "            #plt.savefig(fileName,bbox_inches='tight')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "eb77f23d-38f5-4568-8e42-695a107d3df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ad86a-e8bd-49a1-a2ae-6566b305171f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### data_path = 'C:/Users/sebbas/PycharmProjects/pythonProject/DELTA-Workshop2024/Projections/'\n",
    "#\n",
    "# Case study A - Chunk size 50\n",
    "chunkSizes=[50,100,250]\n",
    "caseStudies=[\"A\",\"B\",\"C\"]\n",
    "lengths=[399,199,79]\n",
    "Methods=[\"SOM\",\"PCA\",\"KernelPCA\"]\n",
    "length=[79,199,399]\n",
    "boundA=0.05\n",
    "boundA=0.1\n",
    "i=2\n",
    "j=2\n",
    "Methods=[\"SOM\",\"PCA\",\"KernelPCA\"]\n",
    "\n",
    "\n",
    "#pValueSignal[0,:]=np.load(data_path+\"pValueSOM_\"+str(chunkSizes[i])+\"_Case_\"+caseStudy+\".npy\")\n",
    "#pValueSignal[1,:]=np.load(data_path+\"pValuePCA_\"+str(chunkSizes[i])+\"_Case_\"+caseStudy+\".npy\")\n",
    "#pValueSignal[2,:]=np.load(data_path+\"pValueKernelPCA_\"+str(chunkSizes[i])+\"_Case_\"+caseStudy+\".npy\")\n",
    "\n",
    "print(len(np.where(np.load(data_path+\"pValueSOM_\"+str(chunkSizes[i])+\"_Case_\"+caseStudy+\".npy\")<=0.05)[0]))\n",
    "print(len(np.where(np.load(data_path+\"pValuePCA_\"+str(chunkSizes[i])+\"_Case_\"+caseStudy+\".npy\")<=0.05)[0]))\n",
    "print(len(np.where(np.load(data_path+\"pValueKernelPCA_\"+str(chunkSizes[i])+\"_Case_\"+caseStudy+\".npy\")<=0.05)[0]))\n",
    "#.\n",
    "\n",
    "for caseStudy in caseStudies:\n",
    "    for chunkSize in chunkSizes:\n",
    "            print(caseStudy, \"&\", chunkSize, \"&\",  Methods[0], \"&\", \"- &\", len(np.where(np.load(data_path+\"pValueSOM_\"+str(chunkSize)+\"_Case_\"+caseStudy+\".npy\")<=0.05)[0]), \"&\",len(np.where(np.load(data_path+\"pValueSOM_\"+str(chunkSize)+\"_Case_\"+caseStudy+\".npy\")<=0.1)[0]))\n",
    "            print(caseStudy, \"&\",  chunkSize, \"&\", Methods[1], \"&\", \"- &\", len(np.where(np.load(data_path+\"pValuePCA_\"+str(chunkSize)+\"_Case_\"+caseStudy+\".npy\")<=0.05)[0]), \"&\", len(np.where(np.load(data_path+\"pValuePCA_\"+str(chunkSize)+\"_Case_\"+caseStudy+\".npy\")<=0.1)[0]))\n",
    "            print(caseStudy, \"&\",  chunkSize, \"&\", Methods[2], \"&\", \"- &\", len(np.where(np.load(data_path+\"pValueKernelPCA_\"+str(chunkSize)+\"_Case_\"+caseStudy+\".npy\")<=0.05)[0]), \"&\", len(np.where(np.load(data_path+\"pValueKernelPCA_\"+str(chunkSize)+\"_Case_\"+caseStudy+\".npy\")<=0.1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e41181-4ae1-4a0d-8c16-f5cdbd19df81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
